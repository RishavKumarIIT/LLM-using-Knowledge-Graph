# -*- coding: utf-8 -*-
"""Utils1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1NBFuJ7S_qpUDBzfHc5VVSUBh_sXwNV
"""

import spacy
import torch
import networkx as nx
from textblob import TextBlob
from difflib import get_close_matches

class QueryGraphExtractor(torch.nn.Module):
    def __init__(self, predefined_relations=None, hops: int = 1):
        super().__init__()
        #self.kg = kg
        self.hops = hops
        self.predefined_relations = predefined_relations or {"has_symptom", "treated_with", "prescription of", "medicines for", "symptom_of"}
        self.nlp = spacy.load("en_core_web_sm")

        # Preprocess KG nodes for case-insensitive matching
        #self.kg_nodes_lower = {node.lower(): node for node in self.kg.nodes}

    def correct_spelling(self, text: str):
        return str(TextBlob(text).correct())

    def extract_entities_relations(self, query: str):
        query = self.correct_spelling(query)
        doc = self.nlp(query)

        candidate_entities = set(ent.text for ent in doc.ents) | set(chunk.text for chunk in doc.noun_chunks)

        matched_entities = [
            self.kg_nodes_lower[entity.lower()] for entity in candidate_entities
            if entity.lower() in self.kg_nodes_lower
        ]

        if not matched_entities:
            for entity in candidate_entities:
                close_matches = get_close_matches(entity.lower(), self.kg_nodes_lower.keys(), n=1, cutoff=0.6)
                matched_entities.extend([self.kg_nodes_lower[match] for match in close_matches])

        candidate_relations = set(token.lemma_ for token in doc if token.pos_ in ["VERB", "ADP"])
        for relation in self.predefined_relations:
            if relation in query:
                candidate_relations.add(relation)

        matched_relations = set()
        kg_relations = [data["relation"] for _, _, data in self.kg.edges(data=True) if "relation" in data]
        for rel in candidate_relations:
            close_matches = get_close_matches(rel, kg_relations, n=1, cutoff=0.6)
            matched_relations.update(close_matches)

        return matched_entities, list(matched_relations)

    def retrieve_subgraph(self, entities: list, relations: list):
        subgraph_nodes = set()
        subgraph_edges = []

        for entity in entities:
            if entity in self.kg:
                neighbors = nx.single_source_shortest_path_length(self.kg, entity, cutoff=self.hops)
                subgraph_nodes.update(neighbors.keys())

        if relations and False:
            for u, v, data in self.kg.edges(data=True):
                if u in subgraph_nodes and v in subgraph_nodes:
                    if "relation" in data and data["relation"] in relations:
                        subgraph_edges.append((u, v, data))
        else:
            for u in subgraph_nodes:
                for v in self.kg.neighbors(u):
                    edge_data = self.kg.get_edge_data(u, v, default={})
                    subgraph_edges.append((u, v, edge_data))

        subgraph = nx.Graph()
        subgraph.add_nodes_from(subgraph_nodes)
        subgraph.add_edges_from(subgraph_edges)
        return subgraph

    def forward(self,kg: nx.Graph, query: str):
        self.kg = kg
        self.kg_nodes_lower = {node.lower(): node for node in self.kg.nodes}
        entities, relations = self.extract_entities_relations(query)
        subgraph = self.retrieve_subgraph(entities, relations)
        return subgraph, entities, relations

import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossModalityPooling(nn.Module):
    def __init__(self, graph_dim, text_dim, common_dim=128):
        super(CrossModalityPooling, self).__init__()
        self.graph_proj = nn.Linear(graph_dim, common_dim)
        self.text_proj = nn.Linear(text_dim, common_dim)

    def forward(self, graph_embeddings, text_embedding):
        """
        graph_embeddings: Tensor of shape [N, graph_dim] (e.g., [100, 64])
        text_embedding: Tensor of shape [text_dim] (e.g., [384])
        """
        # Project both modalities to a common space
        graph_proj = self.graph_proj(graph_embeddings)                # → [N, common_dim]
        text_proj = self.text_proj(text_embedding).unsqueeze(0)       # → [1, common_dim]

        # Attention: dot-product (scaled)
        attn_scores = torch.softmax(torch.matmul(graph_proj, text_proj.T) / (graph_proj.shape[-1] ** 0.5), dim=0)  # [N, 1]

        # Apply attention over graph embeddings
        pooled = (attn_scores * graph_proj).sum(dim=0)  # → [common_dim]

        return pooled  # Final fused representation

class DomainProjector(nn.Module):
    def __init__(self, input_dim=128, domain_dim=128, dropout=0.1):
        super(DomainProjector, self).__init__()
        self.projector = nn.Sequential(
            nn.Linear(input_dim, domain_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.LayerNorm(domain_dim)
        )

    def forward(self, x):
        return self.projector(x)

class LinkPredictionHead(nn.Module):
    def __init__(self, embed_dim=128, hidden_dim=64):
        super(LinkPredictionHead, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # Output score ∈ [0,1]
        )

    def forward(self, head_embed, tail_embed):
        interaction = torch.cat([head_embed, tail_embed, head_embed * tail_embed], dim=-1)
        score = self.mlp(interaction)
        return score.squeeze(-1)  # Return a scalar score

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import get_peft_model, LoraConfig, PeftModel

class GNPModel(nn.Module):
    def __init__(self, gnn_encoder, text_encoder, cross_modal_pooling, domain_projector, link_predictor, base_model_id="google/flan-t5-base", lora_path=None):
        super(GNPModel, self).__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.global_mapping = {}
        self.kg_extractor = QueryGraphExtractor().to(self.device)
        self.gnn_encoder = gnn_encoder.to(self.device)
        self.text_encoder = text_encoder.to(self.device)
        self.cross_modal_pooling = cross_modal_pooling.to(self.device)
        self.domain_projector = domain_projector.to(self.device)
        self.link_predictor = link_predictor.to(self.device)

        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id).to(self.device)

        if lora_path:
            self.llm = PeftModel.from_pretrained(base_model, lora_path).to(self.device)
        else:
            lora_config = LoraConfig(task_type="SEQ_2_SEQ_LM", r=8, lora_alpha=32, lora_dropout=0.1)
            self.llm = get_peft_model(base_model, lora_config).to(self.device)

    @staticmethod
    def format_subgraph_to_text(subgraph):
        facts = []
        for u, v, data in subgraph.edges(data=True):
           rel = data.get("relation", "related_to")
           facts.append(f"{u} {rel} {v}")
        return " | ".join(facts)

    def forward(self,G, query):
        subgraph, entities, relations = self.kg_extractor(G,query)
        if subgraph.number_of_nodes() == 0 or subgraph.number_of_edges() == 0:
            return "No relevant knowledge found in the graph for this query."

        graph_data = self.gnn_encoder.convert_nx_to_torch_data(subgraph, 64).to(self.device)
        if graph_data.edge_index.size(1) == 0:
            return "Empty edge_index after conversion, cannot process GNN."

        gnn_output = self.gnn_encoder(graph_data.x, graph_data.edge_index)
        text_embedding = self.text_encoder(query).to(self.device)

        pooled_representation = self.cross_modal_pooling(gnn_output, text_embedding)
        projected = self.domain_projector(pooled_representation)

        # Ensure shape and device compatibility
        projected = projected.to(self.device)
        graph_text = self.format_subgraph_to_text(subgraph)
        prompt = f"Based on the following medical knowledge: {graph_text}\n\nQuestion: {query}\nAnswer:"
        print("prompt ",prompt)
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(self.device)

        #inputs = self.tokenizer(query, return_tensors="pt", padding=True).to(self.device)

        input_ids = inputs.input_ids
        input_embeds = self.llm.get_input_embeddings()(input_ids)

        # Ensure dtype and device match
        if projected.dtype != input_embeds.dtype:
            projected = projected.to(dtype=input_embeds.dtype)

        prompt_vector = projected.unsqueeze(0).unsqueeze(1)  # shape [1, 1, emb_dim]
        prompt_vector = prompt_vector.to(self.device)

        final_input = torch.cat([prompt_vector, input_embeds], dim=1)

        attention_mask = torch.ones(final_input.shape[:-1], dtype=torch.long).to(self.device)
        # Generate output
        outputs = self.llm.generate(
            inputs_embeds=final_input,
            #input_ids=inputs.input_ids,
            attention_mask=attention_mask,
            max_new_tokens=128,
            num_beams=4,
            no_repeat_ngram_size=2,
            early_stopping=True
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)